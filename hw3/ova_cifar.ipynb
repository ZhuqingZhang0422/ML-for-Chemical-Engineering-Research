{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing OVA logistic regression for the CIFAR-10 dataset\n",
    "In this assignment, you will implement a one-vs-all logistic regression classifier, and apply it to a version of the CIFAR-10 object recognition dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash datasets/get_datasets.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CIFAR-10 dataset\n",
    "Open up a terminal window and navigate to the **datasets** folder inside the  **hw3** folder. Run the\n",
    "**get\\_datasets.sh**  script. On my Mac, I just type in **./get\\_datasets.sh** at the shell prompt.\n",
    "A new folder called **cifar\\_10\\_batches\\_py** will be created and it will contain $50000$ labeled\n",
    "images for training and $10000$ labeled images for testing. The function further partitions the $50000$ training \n",
    "images into a train set and a validation set for selection of hyperparameters. We have provided a function to\n",
    "read this data in **utils.py**. Each image is a $32 \\times 32$ array of RGB triples. It is preprocessed by\n",
    "subtracting the mean image from all images. We flatten each image into a 1-dimensional array of size\n",
    "3072 (i.e., $32\\times 32 \\times 3$). Then a 1 is appended to the front of that vector to handle \n",
    "the intercept term.  So the training set is a numpy matrix of size $49000\\times 3073$, \n",
    "the validation set is a matrix of size $1000\\times 3073$ and the set-aside test set \n",
    "is of size $10000\\times 3073$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "# Get the CIFAR-10 data broken up into train, validation and test sets\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = utils.get_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a one_vs_all classifier for CIFAR-10\n",
    "In this part of the exercise, you will implement one-vs-all classifier by training multiple regularized binary logistic regression classifiers, one for each of the ten classes in our dataset. You should now complete the code in **one\\_vs\\_all.py** to train one classifier for each class. In particular, your code should return all the classifier parameters in a matrix $\\Theta \\in \\Re^{(d+1) \\times K}$,  where each column of $\\Theta$ corresponds to the learned logistic regression parameters for a class. You can do this with a for-loop from $0$ to $K − 1$, training each classifier independently.\n",
    "When training the classifier for class $k \\in \\{0, . . . , K − 1\\}$, you should build a new label for each example $x$ as follows: label $x$ as 1 if $x$ belomgs to class $k$ and zero otherwise. You can use sklearn's logistic regression function to learn each classifier. \n",
    "\n",
    "This function will take about an hour to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from one_vs_all import one_vs_allLogisticRegressor\n",
    "\n",
    "ova_logreg = one_vs_allLogisticRegressor(np.arange(10))\n",
    "\n",
    "# train \n",
    "reg = 1e5\n",
    "ova_logreg.train(X_train,y_train,reg)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = ova_logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print ('one_vs_all on raw pixels final test set accuracy: %f' % (test_accuracy, ))\n",
    "print (confusion_matrix(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the learned one-vs-all classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "theta = ova_logreg.theta[1:,:].T # strip out the bias term\n",
    "theta = theta.reshape(10, 32, 32, 3)\n",
    "\n",
    "theta_min, theta_max = np.min(theta), np.max(theta)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  thetaimg = 255.0 * (theta[i].squeeze() - theta_min) / (theta_max - theta_min)\n",
    "  plt.imshow(thetaimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing your functions with sklearn's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "# train on train set with reg\n",
    "sklearn_ova = OneVsRestClassifier(linear_model.LogisticRegression(C=1.0/reg,penalty='l2',\n",
    "                                                                  fit_intercept=False,solver='lbfgs'))\n",
    "sklearn_ova.fit(X_train, y_train)     \n",
    "\n",
    "# predict on test set\n",
    "y_test_pred_sk = sklearn_ova.predict(X_test)\n",
    "\n",
    "sk_test_accuracy = np.mean(y_test == y_test_pred_sk)\n",
    "print ('one_vs_all on raw pixels final test set accuracy (sklearn): %f' % (sk_test_accuracy, ))\n",
    "print (confusion_matrix(y_test,y_test_pred_sk))                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the sklearn OVA classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "\n",
    "theta = sklearn_ova.coef_[:,1:].T # strip out the bias term\n",
    "theta = theta.reshape(10, 32, 32, 3)\n",
    "\n",
    "theta_min, theta_max = np.min(theta), np.max(theta)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  thetaimg = 255.0 * (theta[i].squeeze() - theta_min) / (theta_max - theta_min)\n",
    "  plt.imshow(thetaimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== START GRADING\n",
      "----- START PART 6.1\n",
      "Expected '[[-1.89253666e-02 -1.91131332e-02 -1.88036727e-02 ... -1.89681927e-02\n",
      "  -1.87712912e-02 -1.86992514e-02]\n",
      " [ 1.77057931e-04  1.44074317e-03  1.19695639e-04 ...  9.48960919e-04\n",
      "  -1.01884056e-03 -2.75492986e-04]\n",
      " [-1.94336648e-04 -2.62943519e-04 -3.19417515e-04 ...  9.50753376e-04\n",
      "  -2.12842777e-04 -3.15787728e-04]\n",
      " ...\n",
      " [-2.12714131e-04  6.23129212e-04  6.48389345e-04 ...  6.07677282e-05\n",
      "  -9.20057004e-04 -4.40682935e-04]\n",
      " [-1.65230985e-04 -4.42896335e-04 -2.42305804e-04 ...  1.45693952e-03\n",
      "  -2.85044534e-04 -8.73666407e-04]\n",
      " [ 6.85264488e-04 -3.08408687e-04 -5.02577261e-04 ...  2.24570299e-05\n",
      "   7.41888783e-04  1.27926231e-04]]', but got '[[-1.90786355e-02 -1.92777891e-02 -1.89770194e-02 ... -1.91003954e-02\n",
      "  -1.89095407e-02 -1.88774188e-02]\n",
      " [ 1.77341740e-04  1.44410945e-03  1.24493564e-04 ...  9.49689948e-04\n",
      "  -1.02161388e-03 -2.75989354e-04]\n",
      " [-1.95244505e-04 -2.65633881e-04 -3.19224954e-04 ...  9.53463318e-04\n",
      "  -2.13599071e-04 -3.17802762e-04]\n",
      " ...\n",
      " [-2.09487698e-04  6.27182578e-04  6.48095571e-04 ...  5.95173952e-05\n",
      "  -9.24729500e-04 -4.40560910e-04]\n",
      " [-1.64683739e-04 -4.44384191e-04 -2.41773579e-04 ...  1.45699407e-03\n",
      "  -2.82830440e-04 -8.74804511e-04]\n",
      " [ 6.87702618e-04 -3.10945657e-04 -5.02007363e-04 ...  2.23857316e-05\n",
      "   7.47951718e-04  1.29652144e-04]]'\n",
      "----- END PART 6.1 [took 0:01:11.546990, 0/10 points]\n",
      "----- START PART 6.2\n",
      "----- END PART 6.2 [took 0:00:00.006100, 5/5 points]\n",
      "----- START PART 7.1\n",
      "Expected to be < 0.050000, but got 0.051641\n",
      "----- END PART 7.1 [took 0:00:17.682330, 0/5 points]\n",
      "----- START PART 7.2\n",
      "----- END PART 7.2 [took 0:00:02.521160, 5/5 points]\n",
      "----- START PART 7.3.1\n",
      "----- END PART 7.3.1 [took 0:00:00.024035, 5/5 points]\n",
      "----- START PART 7.3.2\n",
      "----- END PART 7.3.2 [took 0:00:00.861126, 5/5 points]\n",
      "----- START PART 7.4\n",
      "----- END PART 7.4 [took 0:00:00.024221, 5/5 points]\n",
      "----- START PART 7.5.1\n",
      "iteration 0 / 4000: loss 13.218530\n",
      "iteration 100 / 4000: loss 11.594186\n",
      "iteration 200 / 4000: loss 10.940917\n",
      "iteration 300 / 4000: loss 10.984195\n",
      "iteration 400 / 4000: loss 10.634596\n",
      "iteration 500 / 4000: loss 10.153618\n",
      "iteration 600 / 4000: loss 9.896857\n",
      "iteration 700 / 4000: loss 9.890323\n",
      "iteration 800 / 4000: loss 9.245685\n",
      "iteration 900 / 4000: loss 9.574678\n",
      "iteration 1000 / 4000: loss 9.513895\n",
      "iteration 1100 / 4000: loss 9.234216\n",
      "iteration 1200 / 4000: loss 9.108852\n",
      "iteration 1300 / 4000: loss 9.234742\n",
      "iteration 1400 / 4000: loss 9.375263\n",
      "iteration 1500 / 4000: loss 8.753897\n",
      "iteration 1600 / 4000: loss 8.709556\n",
      "iteration 1700 / 4000: loss 8.535106\n",
      "iteration 1800 / 4000: loss 8.422006\n",
      "iteration 1900 / 4000: loss 8.364989\n",
      "iteration 2000 / 4000: loss 8.334147\n",
      "iteration 2100 / 4000: loss 8.133894\n",
      "iteration 2200 / 4000: loss 8.016893\n",
      "iteration 2300 / 4000: loss 8.039310\n",
      "iteration 2400 / 4000: loss 7.926046\n",
      "iteration 2500 / 4000: loss 7.809884\n",
      "iteration 2600 / 4000: loss 7.854341\n",
      "iteration 2700 / 4000: loss 7.649803\n",
      "iteration 2800 / 4000: loss 7.595063\n",
      "iteration 2900 / 4000: loss 7.424812\n",
      "iteration 3000 / 4000: loss 7.546808\n",
      "iteration 3100 / 4000: loss 7.265399\n",
      "iteration 3200 / 4000: loss 7.268024\n",
      "iteration 3300 / 4000: loss 7.382968\n",
      "iteration 3400 / 4000: loss 7.321756\n",
      "iteration 3500 / 4000: loss 7.061188\n",
      "iteration 3600 / 4000: loss 7.272278\n",
      "iteration 3700 / 4000: loss 6.876658\n",
      "iteration 3800 / 4000: loss 7.188535\n",
      "iteration 3900 / 4000: loss 6.693778\n",
      "----- END PART 7.5.1 [took 0:00:07.245914, 1/1 points]\n",
      "----- START PART 7.5.2\n",
      "----- END PART 7.5.2 [took 0:00:00.012463, 1/1 points]\n",
      "----- START PART 7.5.3\n",
      "----- END PART 7.5.3 [took 0:00:00.000079, 3/3 points]\n",
      "========== END GRADING [30/45 points]\n",
      "grader-manual.out already exists\n",
      "Total max points: 80\n"
     ]
    }
   ],
   "source": [
    "%run grader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
